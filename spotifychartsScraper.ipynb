{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spotifychartsScraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+N11HqjFpMTFZz5OgEN9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChahatKumar/SpotifyChartsScraper/blob/main/spotifychartsScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloudscraper \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjY_L0NfOvGd",
        "outputId": "6e808f21-9183-428d-c031-3d165f72390a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cloudscraper\n",
            "  Downloading cloudscraper-1.2.58-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.23.0)\n",
            "Collecting requests-toolbelt>=0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 40 kB 42.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 51 kB 47.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (1.24.3)\n",
            "Installing collected packages: requests-toolbelt, cloudscraper\n",
            "Successfully installed cloudscraper-1.2.58 requests-toolbelt-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cloudscraper\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import timedelta, date\n",
        "\n",
        "# It generates a list of dates between Jan 1, 2017 and today\n",
        "# in YYYY-MM-DD format\n",
        "scraper = cloudscraper.create_scraper()  # returns a CloudScraper instance\n",
        "\n",
        "\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int ((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "# It creates the list of page links we will get the data from.\n",
        "def create_links(country):\n",
        "    start_date = date(2022, 1, 1)\n",
        "    end_date = date(2022, 1, 2)\n",
        "    links = []\n",
        "    dates = daterange(start_date, end_date)\n",
        "    for single_date in daterange(start_date, end_date):\n",
        "        links.append('https://spotifycharts.com/regional/' + country + '/daily/' + single_date.strftime(\"%Y-%m-%d\"))\n",
        "    return(links, dates)\n",
        "\n",
        "\n",
        "# It reads the webpage.\n",
        "def get_webpage(link):\n",
        "    page = scraper.get(link)\n",
        "    soup = bs(page.content, 'html.parser')\n",
        "    return(soup)\n",
        "\n",
        "# It collects the data for each country, and write them in a list.\n",
        "# The entries are (in order): Song, Artist, Date, Play Count, Rank\n",
        "def get_data(country):\n",
        "    [links, dates] = create_links(country);\n",
        "    rows = []\n",
        "\n",
        "    for (link, date) in zip(links, dates):\n",
        "        soup = get_webpage(link)\n",
        "        entries = soup.find_all(\"td\", class_ = \"chart-table-track\")\n",
        "        streams = soup.find_all(\"td\", class_=\"chart-table-streams\")\n",
        "        for i, (entry, stream) in enumerate(zip(entries,streams)):\n",
        "            song = entry.find('strong').get_text()\n",
        "            artist = entry.find('span').get_text()[3:]\n",
        "            play_count = stream.get_text()\n",
        "            rows.append([song, artist, date, play_count, i+1])\n",
        "\n",
        "    return(rows)\n",
        "\n",
        "# It exports the data for each country in a csv format.\n",
        "# The column names are Song, Artist, Date, Streams, Rank.\n",
        "def save_data(country):\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "    file_name = 'data/' + country[1].replace(\" \", \"_\").lower() + '.csv'\n",
        "    data = get_data(country[0])\n",
        "    if(len(data)!= 0):\n",
        "        data = pd.DataFrame(data, columns=['Song','Artist','Date', 'Streams','Rank'])\n",
        "        data.to_csv(file_name, sep=',', float_format='%s', index = False)\n",
        "\n",
        "# It generates a list of countries for which the data is provided.\n",
        "def get_countries():\n",
        "    page = scraper.get('https://spotifycharts.com/regional')\n",
        "    soup = bs(page.content, 'html.parser')\n",
        "    countries = []\n",
        "    ctys = soup.find('ul').findAll(\"li\")\n",
        "    for cty in ctys:\n",
        "        countries.append([cty[\"data-value\"],cty.get_text()])\n",
        "    return(countries)\n",
        "\n",
        "# It runs the function save_data for each country.\n",
        "# In other words, it creates the .csv data files for each country.\n",
        "def scrape_data():\n",
        "    countries = get_countries()\n",
        "    for country in countries:\n",
        "        save_data(country)\n",
        "\n",
        "scrape_data()"
      ],
      "metadata": {
        "id": "q35ozNEBlS9w"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}